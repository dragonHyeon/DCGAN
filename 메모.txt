nn.ConvTranspose2d vs nn.Upsample
Upsampling: 보간법 사용하는 수동적인 feature engineering, network 가 알 수 없음
ConvTranspose2d: 학습 가능한 parameter
실제로 .parameters() 해서 순회해보면 Upsample 에서는 아무것도 나오지 않음
ConvTranspose2d 에서는 파라미터 값들 쭉 출력됨

nn.BCELoss()
클래스가 두 개인 경우의 cross entropy loss
torch 에서 제공하는 cross entropy loss 는 softmax 가 포함되어있는데 반해 bceloss 는
softmax 가 포함되어 있지 않음. 따라서 nn.BCELoss()(input, target)의 input 에 대해
미리 sigmoid 나 softmax 를 적용하여 0 과 1 사이의 값으로 (합하면 1이 되는 값 이지만 bceloss
이므로 값이 애초에 하나임. 결국 그냥 하나의 확률값으로 나오게 됨) 변형해준 뒤 넣어주어야 함.
input 에 0 과 1 사이 값 안들어가면 오류남
RuntimeError: all elements of input should be between 0 and 1

nn.BCEWithLogitsLoss() 이거를 사용하면 input 에 sigmoid 적용시켜줌. 그래서 이거 사용하면 됨.
그리고 이게 sigmoid 쓰고 bceloss 쓰는 방법보다 더 안정적이라고 함. 이 경우는 input 값이 1000 이런게 들어오면
내부적으로 알아서 1에 근접한 값으로 바뀌게 될 것임. sigmoid 함수 적용되니까

하지만 나는 model 에서 결과 값의 0~1 정도 보기 위해 그냥 bceloss 쓰고 모델에서 sigmoid
적용하는 방식 선택

.detach()
Discriminator 학습 시킬때 fake image 에 대해서 detach() 해줘야 함. detach 안하면 fake image 만든 generator 에도 가중치 학습이 전달 되는데 그러면 안됨
Discriminator 는 그저 만들어지 이미지 그 자체만을 보고 학습을 해야지 그걸 만든 generator 에 가지 영향을 주면 안됨.
generator 학습은 generator 가 discriminator 속이는 과정에서 더 잘 속이는 이미지 만들도록 학습하려 할 때 볼 것임
+ 추가
backward 할 경우 해당 backward 에 사용된 그래프들은 (retain_graph=False 이기 때문에) 다시 for loop 를 돌며 dataloader 에서 다음 데이터를 받기 전까지는  모두 제거된다
따라서 .detach() 하지 않는 경우 generator 학습시 해당 generator 를 update 하는데 필요한 그래프들이 discriminator backward 할때 이미 사용되어 사라진 상태이므로 더이상 사용할 수 없게 되어 학습이 진행되지 않는다.
따라서 애초에 필요도 없는 generator 부분을 discriminator 학습할때 detach 시켜놓는 것이다.
detach 하지 않을거라면 최소한 retain_graph=True 라도 해놔야 한다 (예상 및 stackoverflow 추천 0 답변)
+ 추가
discriminator 에서 retain_graph=true 안해줘도 되는 이유는 chatgpt 에 의하면 원래는 해주는게 좋지만 계산해야 하는 graph 의 크기가 작으면 그냥 즉석에서 다시 그래프 생성해서 계산한다고 함
+ 추가
discriminator 에서 retain_graph=true 안해줘도 되는 이유는 discriminator 에서 두 번의 순전파가 일어나게 되면서 각각의 그래프가 생성되기 때문이다.
따라서 둘이 개별적인 그래프이고 각각 backward 를 진행해서 그래프가 사라져도 서로에게 영향이 없는것이다.
chatgpt 답변은 잘못된 답변
+ 추가
마찬가지로 detach 안한 상태에서 fake_x 를 새로 선언하지 않는다면 오류 발생
RuntimeError: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward.

backward 전에는 zero_grad 하더라도 print(self.modelG.main[0].weight.grad[0][0]) 해도 가중치 볼 수 없음.
None 이기 때문에 (zero_grad 한 뒤로도 0 이 아닌 None 으로 나옴)
backward 한 번 진행해야 grad 값이 None 이 아닌 어떤 값이 나오고 그 이후에 zero_grad 하면 grad 값이 0 으로 나오는 것이다.
그리고 그 값은 계속해서 쌓인다 zero_grad 를 하기 전까지는

zero_grad()
zero_grad 실수로 빼먹었더니 학습 엉망진창으로 진행이 안됐었음.

Normalize
torchvision 에서 Normalize 하는 내용
ToTensor 하면 기본적으로 0~1 값으로 normalize 된다
여기서 standardization (z-score 방식) 처럼 하기 위해 normalize 추가적으로 붙이는것으로 보임
RGB 채널 별로 적용을 하기 위해 3 개의 값 전달하는 것임
mean 및 std 에 0.5 로 주는것은 임의의 값을 지정한 것 뿐이다
흔히 사용되는 (0.485, 0.456, 0.406), (0.229, 0.224, 0.225) 값이 있는데 이는 ImageNet 에서 ToTensor 통해 0~1 값으로 변환한 뒤 mean, std 값 계산해보니 저런 값이 나왔다고 함
아무튼 어떤 값을 사용하든 해당 값을 평균, 표준편차라고 가정을하고 표준화하는 것이기 때문에 평균, 표준편차가 실제 값과 정확히 맞지 않아도 괜찮을 것으로 예상됨
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers)
이 문제는 데이터 정규화 하고 시각화 과정에서 정규화 했던걸 원래대로 돌려놓지 않아서 (역정규화하지 않아서) 발생했던 문제였음
역정규화 하니 오류 사라짐
아마 정규화 하고나면 값이 대략 -1 ~ 1 사이 값을 가질텐데 오류 메세지대로 float 인 경우 값이 0 ~ 1 사이 값을 가져야해서 변환 과정이 진행 되는듯

<item>
# BCE loss 계산
batch_bce_loss = F.binary_cross_entropy(input=output, target=label).cpu().detach().numpy()
여기서 .cpu().detach().numpy() 는 추후에 값 평균 낼때 numpy 함수 이용하려고 numpy 로 바꾸기 위함인데 이럴필요 없이 그냥 .item() 으로 처리해줘도 됨
batch_bce_loss = F.binary_cross_entropy(input=output, target=label).item()
